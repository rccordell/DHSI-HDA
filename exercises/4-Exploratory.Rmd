---
title: "Exploratory Data Analysis"
author: "Ryan Cordell and Jonathan Fitzgerald"
date: "2/1/2017"
output: html_document
---

Load `tidyverse` before we get started, and make sure your working directory is set correctly (`getwd()` and `setwd()`).

```{r}
library(tidyverse)
```

# The Pipe Operator

Last week we explictly invoked each function in a new line of code. Today we'll introduce the pipe operator `%>%`, which allows us to chain together a series of transformations. Let's illustrate this with a few familiar blocks from last week:

```{r}
census <- read.csv(file = "./data/1840-census-data.csv")
census <- select(census, 1,6:81)
census <- rename(census, county = QualifyingAreaName)
census_long <- gather(census, "identification", "count", 2:77)
census_long <- separate(census_long, county, into = c("county", "state"), sep = "\\, ")
census_long <- na.omit(census_long)
```

Using pipes, we can chain together these operations like so, to create two variables:

```{r}
census <- read.csv(file = "./data/1840-census-data.csv") %>%
  select(1,6:81) %>%
  rename(county = QualifyingAreaName)

census_long <- census %>%
  gather("identification", "count", 2:77) %>%
  separate(county, into = c("county", "state"), sep = "\\, ") %>%
  na.omit(census_long)
```

Or like so, to create only one `census_long` variable (note that you'll have to clear your `Global Environment` before running this code to see it actually work.)

```{r}
census_long <- read.csv(file = "./data/1840-census-data.csv") %>%
  select(1,6:81) %>%
  rename(county = QualifyingAreaName) %>%
  gather("identification", "count", 2:77) %>%
  separate(county, into = c("county", "state"), sep = "\\, ") %>%
  na.omit(census_long)
```

What did each of these do? We'll talk about that together.

Note that there are some distinct structural differences when using pipes. For one, the variable being transformed is usually invoked at the beginning of the chain, and thus does not appear as an argument in the separate parts of the chain. Compare these piped operations with the line-by-line operations above again. Where do you see `census` or `census_long` invoked in lines 17-22 when they are not in lines 41-46? 

We can also use pipes to make (and view) temporary transformations in our data that won't be saved as variables. This is a very useful way of seeing what a series of operations will do before "really" running them.

```{r}
census_long %>% 
  spread(identification, count) %>%
  View()
```

# Part One, Transforming Data Frames

So what kinds of transformations might we want to make with dataframes? To begin thinking through that question, let's import a small sample of data from the Viral Texts Project. In the code box below, import the CSV `VT-vignettes.csv` from the class data folder (for consistency, let's call the variable `vtClusters`):

```{r}

vtClusters <- read.csv(file = "./data/VT-vignettes.csv")

```

There are *lots* of quirks in this data, which we'll talk through together. I want to use this data to talk about a few more features of dataframes in R and . First you'll notice that unlike our census data from last week, the different columns here comprise many different data types: text, numbers, dates, etc. Data types are important and sometimes frustrating in R; there are functions that can operate on strings (text data) but not other kinds. You can determine the data type of anything in R using the `class()` function:

```{r}

# you can investigate the class of a whole variable:
class(vtClusters)
# or of a variable within a dataframe
class(vtClusters$text)
class(vtClusters$date)
```

Notice anything about the class of our text and date column? By default, when creating dataframes R converts textual data—including numbers with typographical features, like the hyphens in our date column—to `factors`. One of the most human readable definitions of factors can be found at [https://www.r-bloggers.com/data-types-part-3-factors/](https://www.r-bloggers.com/data-types-part-3-factors/). In short, however, factors are wonderful for variables you might want summary statistics about, but not always wonderful for other data types. If we wanted to work with dates *qua* dates—to find out which reprints occurred earlier than others—a factor `date` column might not serve. Likewise for doing certain kinds of text analysis on the `text` column. 

When importing tabular data we can include an argument that will `coerce` (read: force) strings to import as strings rather than factors. There are also functions in R for coercing one data type into another: `as.Date`,  `as.character`, `as.dataframe`, and so forth. Can you identify the two coercions in the code below?

```{r}

vtClusters <- read.csv(file="./data/VT-vignettes.csv", stringsAsFactors=FALSE)
vtClusters$date <- as.Date(vtClusters$date, "%Y-%m-%d")

```

Now check the class of the `text` and `date` columns in the console. Also type `vtClusters` into the console to see what prints.

## Tibbles

Most recently, lots of folks have moved away from base R's dataframes and to "tibbles", which are essentially a new protocol for creating dataframes developed by Hadley Wickham, of Tidyverse fame. You can read more about tibbles using the command `vignette("tibble")` in the console. In most ways they act just like dataframes, and indeed they are a subspecies of dataframe, but they correct a few annoyances of the dataframe. One big one: when importing data into a tibble the data types and column names will not be converted, so you need not convert strings to factors. Other fields, such as dates, might still need to be coerced. 

To convert data into a tibble, you can use the functions `as_data_frame` (with underscores, not periods) or `as_tibble`. You can either convert an existing dataframe or do this upon importing new data. Below we're going to use pipes to import our Viral Texts data as a tibble, coerce the date column into the date class, and do one other thing. Can you tell what that is?

```{r}

vtClusters <- read_csv(file="./data/VT-vignettes.csv") %>% 
  mutate(title = gsub('(.*)\\.*\\(.*','\\1',title)) %>%
  mutate(date = as.Date(date, "%Y-%m-%d"))

```

Now type `vtClusters` into the console again to see what prints. Notice the difference? The print behavior of tibbles is far less loquacious than standard data frames. Before we move on, can you write some code into the code block above so that our final `vtClusters` dataframe includes **only** the `cluster`, `date`, and `title` columns?

## From Bibliography to Network

One of the transformations I most often need for Viral Texts brings us from a list of reprints organized in "clusters"—essentially ennumerative bibliographies—to data that expresses the network relationships among newspapers within those clusters. Essentially I want to reorganize the table so that co-membership in a given cluster creates a line of "edge" data between all the newspapers within that cluster. If that doesn't make sense, run the code below and we'll talk through what it does and the transformation it produces. 

```{r}

vtClusters <- vtClusters[ , c("cluster", "date", "title")]

vtClusters %>%
  mutate(title = gsub('(.*)\\.*\\(.*','\\1',title)) %>%
  mutate(date = as.Date(date, "%Y-%m-%d")) %>%
  full_join(vtClusters, vtClusters, by = "cluster") %>%
  filter(date.x < date.y) %>%
  View()

```

We could also structure things to put the new edges table into a new variable:

```{r}

vtClusters <- as_data_frame(read.csv(file="./data/VT-vignettes.csv")[ , c("cluster", "date", "title")]) %>% 
  mutate(title = gsub('(.*)\\.*\\(.*','\\1',title)) %>%
  mutate(date = as.Date(date, "%Y-%m-%d"))

vtDirected <- full_join(vtClusters, vtClusters, by = "cluster") %>%
  filter(date.x < date.y)
View(vtDirected)

```

So now we have a dataframe expressing every possible edge derived from the cluster data provided. From here we might decide to add even more nuance. In most network graphs, every observed relationship between two entities is assigned a weight of 1, and when two entities are connected multiple times (in this case, when two newspapers are members of multiple clusters) the overall weight of their relationship increases by 1 for each observation. 

Recently, however, I've been experimenting with other ways of weighting relationships based on features in my reprinting data. For instance, how might we adjust our weights based on the time lag between two observed reprintings of the same text? The code below is one experiment I've made along those lines; we can talk through my logic once you run it. 

```{r}

vtEdges <- vtDirected %>%
  mutate(lag = date.y - date.x) %>%
  mutate(lagWeight = 1 / as.numeric(lag)) %>%
  group_by(title.x,title.y) %>%
  summarise(lag =mean(lag), weight = sum(lagWeight), rawWeight = n()) %>%
  mutate(lagEffect = weight - rawWeight) %>%
  arrange(desc(weight)) %>%
  rename(source = title.x, target = title.y)

View(vtEdges)

```

Before we move on, I realize that we haven't yet discussed how to output data *from* Rstudio. Like importing data, there are different functions for exporting different data formats. If we wanted to export a CSV of our network data—to import into network software like Gephi, say—we could do the following:

```{r}

write.csv(vtEdges, file="./output/vtEdges.csv")

```

# Part Two, Exploring Texts

Over the past couple of weeks we've been working with dataframes that contain mostly numerical data and other bits of short textual information. Now we're going to start digging into longer texts, a common subject for humanistic data analysis. We'll begin working with a single text, and then learn how to read in a set of texts. The texts we'll be using are derived from the [Wright American Fiction project](http://webapp1.dlib.indiana.edu/TEIgeneral/projectinfo.do?brand=wright) out of Indiana University. I (Fitz) used this data for a recent classification project for Viral Texts in which we compared vignettes, a popular genre in nineteenth century newspapers, with short fiction and news items to attempt to quantifiably locate the vignette in the space between fiction and news. We're not doing any classification here, but it is an interesting data set to work with.

Let's begin by downloading the folder that contains the texts from Github to your local machine. In the `data` folder on Github you'll find a new subfolder titled `wright-txt`. The easiest way to download an entire folder from Github is to use the "Clone or download" button from the main page of the repository and choose "Download ZIP." Once you've downloaded the folder, unzip it (if your OS didn't do that for you already), and navigate to the `wright-txt` folder. Copy it into your data folder, and then you can trash the rest of the downloaded folder since it will probably be a duplicate of what you already have.

## Reading in a Single Text

To read in a text, we're going to use the `scan` function as follows:

```{r}

text = scan("data/wright-txt/VAC5884.txt",sep="\n",what="raw")

```

Here, we're creating a new variable called `text` by scanning in the data from the file `VAC5884.txt`. Additional arguments we've included are to separate the text by new line (`\n`) and we're telling that the data type is `raw`, which means, essentially, unformatted text. You can take a look at what was imported by highlighting the word `text` above and hitting `command+enter`. You'll notice each paragraph is a new line, and there are also some blank lines, as indidcated by empty quotation marks.

## Tokenization

Having the full text is nice, but in order to perform some analysis on this text, it will be helpful to break the text up into words. Breaking a piece of text into words is called "tokenizing." There are many ways to do it, but the simplest is to simply to remove anything that isn't a letter. Using regular expression syntax, the R function `strsplit` lets us do just this: split a string into pieces. We'll use the regular expression `[^A-Za-z]` to say "split on anything that isn't a letter between A and Z." Note, for example, that this makes the word "Don't" into two words.

```{r}

test =text %>% 
  strsplit("[^A-Za-z]")

```

## Converting from List to Character

You'll notice that now each paragraph is broken off in a strange way. The above function created a list in which the paragraph (or line) is still the top level and nested below is another list of all the words in that paragraph. If we use the `unlist` function, we convert this from a list to a character string that includes all the words.

```{r}

words = text %>% 
  strsplit("[^A-Za-z]") %>% 
  unlist 

```

## Converting from Character to Dataframe

Let's take this one step further and coerce the data into a format we're more used to working with, a dataframe:

```{r}

wrightWords = data.frame(word=words,stringsAsFactors = "false")

```

Now we've created a dataframe called `wrightWords`. The `stringsAsFactors` argument tells it that we don't want to convert the strings to factors and instead want to leave them as characters. We could also do this, as above, by creating a tibble.

```{r}

wrightWords = tibble(word=words)

```

## Removing Blank Observations

If you look at that dataframe, you'll notice that each word appears in its own row or observation, but there are also a lot blank observations. This is because of all the spaces in the original document. Let's use a filter to get rid of all of those:

```{r}

wrightWords = wrightWords %>% filter(word != "")

```

Here, we're overwriting the dataframe with a new dataframe that we've filtered to include only words that are not equal to (`!=`) nothing. This is a good time to explain the syntax you see here for not equal to. In R, as well as in a lot of other programming languages, to indicate that something is equal you use two equal signs (`==`). Not equal, as we've seen, is `!=`. You can also use greater than (`>`), less than (`<`), greater than or equal to (`>=`), or less than or equal to (`<=`).

## Word Counts

Okay, now that we have this in a dataframe with all the words and no blank spaces, let's start doing some analysis. One of the most basic forms of analysis is counting words. In order to do this we're going to need to pipe together a few functions. First, we'll create a new dataframe called `wordcounts`. Then because we want to count the total number of each word, we'll use `group_by(word)`. Here, `word` is the name of the column, or variable, and just as it says, this arranges variables into groups. Next, we use the `summarize()` function, which summarizes multiple values into a single value, and the single value we want is going to be called `count`. `n()` is a variable that means the number of obserations in a group. Finally, we're using the `arrange()` function to arrange the dataframe by the `count` column in descending order (as indicated by the `-`). Try it out:

```{r}

wordcounts = wrightWords %>% group_by(word) %>% summarize(count=n()) %>% arrange(-count)

```

## Visualizing Word Counts

If you look at that dataframe `head(wordcounts)`, you shouldn't be terribly surprised to find that the most common words are "the", "of", "and", "to, "in", "a", and so on. There are ways to filter out these most common words, which we'll explore later in the semester, but for now we'll let them stand. Now that we have these word counts, let's try something fun. Using the `ggplot2` package (included in `tidyverse`) we can plot the most common words. I'm not going to go into too much detail about what is going on here since we'll be delving more deeply into visualization next week, but let's run the following commands:

```{r}

wordcounts = wordcounts %>% mutate(rank = rank(-count))  %>% filter(count>2,word!="")

ggplot(wordcounts) + aes(x=rank,y=count,label=word) + geom_text() + scale_x_continuous(trans="log") + scale_y_continuous(trans="log")

```

You'll notice we've added a column called `rank` using the `mutate()` function and used the `rank()` function to assign a rank based on the count in descending order. When you plot the count over rank you should see an interesting pattern. The logarithm of rank decreases linearily with the logarithm of count. 

This is "Zipf's law:" the phenomenon means that the most common word is twice as common as the second most common word, three times as common as the third most common word, four times as common as the fourth most common word, and so forth. 

It is named after the linguist George Zipf, who first found the phenomenon while laboriously counting occurrences of individual words in Joyce's *Ulysses* in 1935. Not super relevant, but too interesting to not share.

# Building Concordances

Our last experiment with the words from this singular text will be to create a concordance. Of course, this process used to be the effort of entire scholarly careers, but we can do this by adding another column to the frame which is not just the first word, but the second. `dplyr` includes a `lag` and `lead` function that let you combine the next element. You specify by how many positions you want a vector to "lag" or "lead" another one. Try it below:

``` {r}
numbers = c(1,2,3,4,5)
lag(numbers,1)
lead(numbers,1)
```

We created a list of numbers, 1-5, and then ask it first to "lag" or go back by one, and then "lead" or go forward by one.

By using `lead` on a character vector, we can neatly align one series of text with the words that follow. Below we use `mutate()` again to add another new column, which we call `word2` and indicate that the value of that column should be the value of `word` led by 1.

```{r}

wrightWords %>% mutate(word2 = lead(word,1)) %>% head

```

If we add multiple lead columns we can construct our concordance:

```{r}

multiColumn = wrightWords %>% mutate(word2 = lead(word,1),word3=lead(word,2),word4=lead(word,3))

```

You can get context around a certain word as follows:

```{r}

multiColumn %>% filter(word3=="sea")

```

Apparently, the word "sea" appears twice in the text in phrases such as "the wide sea of" and "the unrelenting sea must".

## Reading in Multiple Texts

Working with a single text is fun, but the real magic happens when you have a set of texts to work with. We begin by creating a list of all the text files we want to read in:

```{r}

WRIGHT = list.files("data/wright-txt",full.names = TRUE)

```

Using the `list.files()` function, we point to the folder where our files are stored and create a list of all the filenames there. 

## Building a Function

Next, we're going to build our own function. This gets a little complicated, but what you really need to know is that a function is used to perform several operations at once. We have used lots of pre-made functions thus far, but R allows you to write you own. We could even save this function in a separate `.R` file and call it from lots of different scripts. This is one way to store operations you use frequently so you don't have to rewrite the code for them each time you need them. We will delve into writing functions more later, but for now, run the following and I'll explain the various parts below:

```{r}

readWRIGHTtext = function(file) {
  message(file)
  text = paste(scan(file, sep="\n",what="raw",strip.white = TRUE))
  WRIGHT = data.frame(filename=file,text=text,stringsAsFactors = FALSE) %>% group_by(filename) %>% summarise(text = paste(text, collapse = " "))
  return(WRIGHT)
}

```

The first thing to note here is that after you run the above code it will look like nothing has happened. But, what _has_ happened is that function has been stored by R for later use. You can see it if you scroll to the bottom of your "Environment" window (to the right for most of you). 

This particular function takes as its starting point the individual file. Then it creates a variable called `text` in which it scans the contents of the file (just like we did above). Then, it creates a kind of temporary dataframe, in this case called `WRIGHT`, wherein there are two columns "filename", which will be the name of the file and "text." 

## Running the Function

In order to run the function, we do the following (and don't forget to enjoy the satisfactory feeling of watching the filenames scroll by in the console!): 

```{r}

allWRIGHTtext = data.frame(filename=WRIGHT,stringsAsFactors=FALSE) %>% 
  group_by(filename) %>% 
  do(readWRIGHTtext(.$filename)) 

```

Here we are creating a new dataframe that uses the temporary dataframe created by the function `WRIGHT` as its starting point, then it pulls together (`group_by(filename)`) all the texts by file name and runs the function (`do()`). The output will be a new dataframe called `allWRIGHTtext` with two columns, "filename" and "text". 

Now is as good a time as any to talk about code reuse. The above function was originally created by Ben Schmidt when I took his HDA course. I've since adapted and used it over and over again in the years since. Maybe this is obvious to you by now, but when writing code there's never a need to reinvent the wheel. 

## Modifying the Function to Tokenize.

Speaking of not reinventing the wheel, you'll notice that the above function outputs the full text of each file, but as above, we probably want to tokenize this to work with words. We can build this feature right into the function with a few small tweaks:

```{r}

readWRIGHTwords = function(file) {
  message(file)
  text = paste(scan(file, sep="\n",what="raw",strip.white = TRUE),collapse = "/n")
  words = text %>% 
    strsplit("[^A-Za-z]") %>% 
    unlist 
  WRIGHT = data.frame(word=words,filename=gsub("data/wright-txt/","",file),stringsAsFactors = FALSE) %>% filter(word != "")
  return(WRIGHT)
}

```

We've created a new function called `readWRIGHTwords` (as opposed to `readWRIGHTtext` as above). You'll notice that in the "functions" section of your environment, this has been added. What have we changed? (note the `gsub()`)

Let's run that new function:

```{r}

allWRIGHTwords = data.frame(filename=WRIGHT,stringsAsFactors=FALSE) %>% 
  group_by(filename) %>% 
  do(readWRIGHTwords(.$filename))

```

# Unique Words

Now that we have a dataframe with all the words for all the files, it might be kind of interesting to see what words are unique to certain texts.

```{r}

unique = allWRIGHTwords %>% 
  mutate(word=tolower(word)) %>%
  distinct(word) %>% 
  group_by(word) %>% 
  filter(n()==1) %>% 
  arrange(word)

```

Here we are piping together a few functions: first, because we want to see the unique words regardless of their case (we want "The" and "the" to be the same word) we use `mutate()` to replace the column "word" with another column "word" in which all words are lowercase. That is what the function `tolower()` does for us. Then, we use another handy function `distinct()` to find unique (or distinct) rows. Finally, we group those distinct words together and filter to select just one. This last step is necessary because a word that is distinct to a text might appear more than once in that text, thus giving us an inaccurate count. 

# Concordance

Above, we created a concordance for the single text. Can you create a concordance for the entire set? BONUS: You'll probably notice that your new dataframe's columns seem a bit out of order, can you arrange them so that they are: filename, word, word2, word3?

```{r}


```

You'll probably notice that your new dataframe's columns seem a bit out of order. Here's a quick trick for reordering columns (I also could've just set the column order properly in the function above):

```{r}

allWRIGHTwords = allWRIGHTwords[,c("filename","word","word2","word3")]

```

## Let's take a random walk

Okay, that's really all we need to do this week, but just for fun, let's try out a trick that Ben Schmidt showed me in his HDA class. We're going to create a "Random Walk Generator". 

First, we need to calculate the probability that one word will follow another. To do this, run the code below:

```{r}

transitions = allWRIGHTwords %>% 
  group_by(word) %>% 
  mutate(word1Count=n()) %>% 
  group_by(word,word2) %>% 
  summarize(chance = n()/word1Count[1])

```

We are creating a new dataframe in which we create a new column `word1Count` which is a count of the number of times a word appears in our data. If you were to run the code only as far as this, you'd see, for example that the word "CITATION" appears 100 times, once at the top of each text. From there, however we group by both the first word and the second word and calculate the chance that the second word will follow the first.

From there we create a function to randomly select a next word based on the probability that it will follow the previous word. 

```{r}

findNextWord = function(current) {
  subset = transitions %>% filter(word==current)
  nextWord = sample(subset$word2,1,prob = subset$chance)
}

```

Finally, when we run the function, we'll see the console begin printing out words that should, if this is working right, make some logical sense (the logic breaks down after a while). Try it out:

```{r}

word = "I"
while(TRUE) {
  message(word)
  word = findNextWord(word)
}

```

In our data, which includes fiction written by different authors (although all nineteenth century writers), this can feel a bit more random, but if you were working with a corpus from a single author, the randomly generated text actually _sounds_ a bit like the author, albeit if the author wrote nonsense. I tried this out with a corpus of Dickens' texts with the shortlived notion that I'd create a twitterbot called Automatic Dickens. I created two tweet-length pieces that read:

>I am much the better he said that he had considered how I must tell you I do carry it

and 

>I suppose he has the main point were as empty and then and likewise to place myself during the cessation

Fun stuff.

# Exercises

1. In the `allWRIGHTtext` dataframe, you'll notice that each text contains metadata about the text before the actual text begins. Using `gsub()` and regular expressions, can you remove the metadata?

2. Below I've pasted a series of operations we performed on the census dataset last week. Can you restructure this into a single operation that uses pipes to reach the same long dataframe?

```{r}
census <- read.csv(file = "./data/1840-census-data.csv")
census <- select(census, 1,6:81)
census <- rename(census, county = QualifyingAreaName)
census_long <- gather(census, "identification", "count", 2:77)
census_long <- separate(census_long, county, into = c("county", "state"), sep = "\\, ")
census_long <- na.omit(census_long)
```

3. What is the longest single text in the `VT-vignettes.csv` dataset? What is the longest overall text by cluster?

4. For this exercise, you should assemble your own corpus of *no fewer than **five** distinct texts* and read them into R as a tibble. What is the most common word in your corpus? What is the most common trigram? Just to make this even more challenging/meaningful, exclude most common words, or "stop words." The `tidytext` package has a built in data frame featuring 1,149 stop words. Do `?stop_words` to learn more.

5. (reach goal) Create a random vignette! Using the method we utilized to create a random walk generator from the Wright texts, create a random vignette from the Viral Texts data. Remember that this process started way back at the beginning with tokenizing the Wright texts, so you'll need to tokenize the vignettes. Then, once you get the random walk generator up and running, stop it when you feel like you have something worth sharing and copy and paste the output here.