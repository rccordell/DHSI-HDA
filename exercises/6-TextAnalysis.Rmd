---
title: "Transformation and Analysis of Textual Data"
author: "Gregory Palermo"
output: html_document
editor_options: 
  chunk_output_type: console
---

So far this week, we've mostly worked with data that's mostly already in a tabular format. This data lends itself well to manipulation and analysis in the `tidyverse`. If we've dealt with text it's been stored in single strings that are easily made a variable in a tibble. It's been fairly straightforward how this data should be structured: each text-based variable probably has a single string that's associated with an observation in the tibble. For example, <add specifics based on Ryan's>

But what happens when the data we have and want to work is in some other form? Humanistic scholarship often deals, for example, with bodies of text: Documents. Transcripts of speeches or proceedings. Books. This text might be stored in a plaintext, or `.txt` file, or a set of them, a corpus. Or, we might be working with text from the web or a project-curated dataset that is "marked up" with tags, which tell us information about its structure or features of potential interest. Or the text we want might be pdf files that have user-selectable text (or maybe that text isn't user-selectable, which is a whole other set of challenges that we won't be able to cover today).

What are some types of textual data we've seen or are working with? How do we get those data it into forms that we know how to deal with?

# Bringing textual data into R

## A Single Text File 
The first corpus we will look at are the transcripts of US Presidential Inaugural Addresses, compiled by C-Span. Work on this was inspired by Taylor Arnold and Lauren Tilton's [Programming Historian tutorial](https://programminghistorian.org/en/lessons/basic-text-processing-in-r#analyzing-every-state-of-the-union-address-from-1790-to-2016) on "Basic Text Processing in R", in which they analyze State of the Union addresses. (The last time this course ran, Trump hadn't made a State of the Union. Maybe now that's worth revisiting...) I've adapted some of the following code from there.

If you look in the `inaugural` folder, there are a bunch of consistently-named text files, and each has only the content of an address with no other identifying information. Each line in the text file is a paragraph of the transcript.

A single `.txt` file can be read into R, and stored in a variable, with the `readLines` function. This function puts out a character vector (a one-dimensional "array" in which each element is a string of text), with elements for each line in the file. Since we'd probably rather have the whole speech as a unit instead, we can use the `paste` function to "collapse" it at every line break. Take a look at the resulting string. What information about line breaks in the original is retained, and how?

```{r}

trump <- paste(readLines("./data/inaugural/2017-Trump.txt"),collapse = "\n")

trump

```

## Importing a Corpus of Text Files

Using the base-R `dir` function, we can tell R to give us a list of filenames in a directory that meet certain criteria. Can you read the regular expression here that tells R the pattern? What kind of files is it bringing in? Why might we specify, instead of just bringing in all the files in the directory?

```{r}
base_url <- "./data/inaugural/"
filenames <- dir(base_url,pattern = "(\\d{4})-([A-Za-z]+).txt")
```

We need this list of file paths in order to tell R where each file is, in order to read it in. Below, the `paste0` lets us "concatenate" (combine) the base URL (the directory where the files live) with the filename for a text file. Here, let's do the first one in the list.

```{r}

paste0(base_url,filenames[1])

```

Then we have a single file path. Of course, we have multiple files: each has the same base_url, but a different filename. If we want to generate a list of filepaths given multiple filenames, we can wrap what we've done in a `sprintf` function. This will add the base URL we've defined to the beginning of every filename on our list, and store it as a new variable.

```{r}
filepaths <- sprintf(paste0(base_url,"%s"),filenames)
```

If we create an empty character vector with the `c` function, we can then loop through the filepaths and tell R to read in each file.

```{r}
inaugural_text <- c()
for (filepath in filepaths){
  inaugural_text <- c(inaugural_text, paste((readLines(filepath)), collapse = "\n"))
  }
```

Looking at the structure of `text` with the `str` function, what do we have?

```{r}
str(inaugural_text)
```

### Associating Text with Metadata

But, uh-oh, we don't know which address is which! In this case, the only identifying informaiton we have about what each file is its name, and the file contains no such information. That identifying metdata is stored instead in an accompanying `.csv` file. Other datasets, however, might have this metadata in a header in each file. (Since this is not really a course in data preparation, we won't cover how to isolate just the text — this is a task, however, that a computer can help us do efficiently.)

The benefit to the separate `.csv` file approach is that we can add to this file later if there are other variables we might find useful to associate with each address, as our interests evolve. The challenge is associating each element in the character vector we've created with its associated metadata.

Calling the `tidyverse` packages, we can load the information in this .csv file in a tibble. What might we put into the `paste0` function below to avoid typing out the whole filepath referencing `metadata.csv`? (Scroll down a bit and see where I defined a variable called `metadata` if you want the answer.)

```{r}
library(tidyverse)

# metadata <- read_csv(paste0(,))

```

In order to associate each speech with its president and year, the Programming Historian Tutorial exploits a symmetry in the data: that the filenames are chronological, and R reads in the files in the same order as the list of files in the metadata. It thus holds off on adding identifying information about each address until it plots its analysis, rejoining data and metadata later in the process.

However, I'd rather not take that for granted. It might be useful to keep track of what comes from where as we move forward (you'll see why soon). This is the perfect place to incorporate our newfound knowledge of tabular data! What if we create a tibble in which each observation was one of the speeches, and we had a variable identifying the filename?

If we rewrite the code above that brings in the corpus's text files as a function, we can then apply it to transform the list of filenames into a list of character vectors. Each of these vectors has the content of the associated speech. 

```{r}
read_address_text = function(filename) {
  address = paste(readLines(paste0(base_url,filename)),collapse="\n")
  return(address)
}

addresses <- filenames %>%
  lapply(read_address_text)
```

Then, since the filenames and the list of character vectors containing the speeches are in the same order, we can match them up in a tibble! This tibble will contain, as its variables, the filename and a list of the speech text character vectors. Since each of these vectors only has a single element, it's probably not convenient to have to ask the computer for the single element of that vector all the time. So, finally, we can passing the tibble we created through the `unnest()` function to create an observation for each element in the character vector associated with each speech; since there's only one element in each of these character vectors, each speech will have a single observation.

```{r}
addresses <- tibble(
  filename = filenames,
  text = addresses) %>%
  unnest()

```

The power here is that this filename can serve as a **unique identifier** for the speech. Now, we can use that to link each speech observation to an observation in our metadata tibble, using a join function. R will automatically find the variable that they share (`filename`).

```{r}
metadata <- read_csv(paste0(base_url,"metadata.csv")) %>%
  mutate(year = as.integer(year))

addresses <- inner_join(addresses,metadata)

```

# Parsing and Tokenizing Text

Now that we've gotten our texts into a data structure, let's zoom in and look at that same single transcript of Trump's speech as structured data. Rather than loading in the text file, let's find the text it in our `addresses` tibble with the `filter` and `select` functions.

```{r}
addresses %>% 
  filter(president == "Trump") %>%
  select(text) %>%
  paste()
```

When you scan this continuous string of text with your eyes, how do you know where a new sentence occurs? How about a paragraph? Going a level in the other direction, how do you know when you're in a new word? How can we tell a computer?

While a parser means something specific in formal computer science speak, we digital humanists talk about **parsing** digitized text the same way we do, for example, if we're speaking colloquially about parsing any object: to parse something is to break it down into parts to understand it. When we **parse,** using a computer, we navigate through a string of characters in order to identify parts of the string and assign names to those parts. Parsing is a form of expressing structure that was formerly implicit, or otherwise represented differently. Like any imposition of structure, it is also a form of boundary work: we have to decide what counts, what doesn't, and what is used to draw the boundaries between them.

One particular type of text parsing is **tokenizing**, in which a text is split into lingusitic units, or "tokens": these can be words, sentences, paragraphs, you name it. Analyzing text at different scales yields different features, which may prompt or be useful for answering different research questions.

While we could use base-R to split text into different units by searching for white space or line breaks, there is no need to reinvent the wheel. Lincoln Mullen has put together a fantastic R package called `tokenizers` with a library of functions that will common ways we commonly parse the components of text.

## Scales

### Tokenizing By Word

Let's start at the word-level.

```{r}
library(tokenizers)

addresses %>%
  group_by(filename) %>%
  mutate(words = tokenize_words(text)) %>%
  select(filename,words)
```

Here, we've created a variable with a list of words associated with each speech. But all we've really done is delimit the words. What happens, instead, if we unnest that variable?

```{r}
addresses %>%
  group_by(filename) %>%
  mutate(words = tokenize_words(text)) %>%
  select(filename,words)%>%
  unnest(words)
```

Wow! We now have an observation for each word, which is also associated with the speech it's in. Note that if we break the speeches down by word, quite a few observations are no longer unique. What information can we potentially lose, should we otherwise sort these? (Let's discuss.)

```{r}
addresses %>%
  group_by(filename) %>%
  mutate(words = tokenize_words(text)) %>%
  select(filename,words)%>%
  unnest(words) %>%
  arrange(desc(words))
```


#### Word Counts and Frequencies

The most basic thing we might look at, here are word frequencies across the whole corpus. In that case, the object in question is each *word*, so that's what we should group by. For convenience, I'm going to define a new variable.

```{r}
address_words <- addresses %>%
  group_by(filename) %>%
  mutate(word = tokenize_words(text)) %>%
  select(filename,word) %>%
  unnest(word)

address_words %>%
  group_by(word)%>%
  summarize(count = n()) %>%
  arrange(-count)
  
```

What are the most frequent words here? We may want to subtract out "stop words" if function words aren't important to us. (In what cases might they be?) First, we'll need to import a list of them. Luckily, the tidyverse has one built in.

```{r}
library(tidytext)
stopwordlist <- stop_words %>%
  filter(lexicon == "SMART") %>%
  select(word)

address_words %>%
  anti_join(stopwordlist)
  group_by(word)%>%
  summarize(count=n()) %>%
  arrange(-count)%>%
  top_n(10)
```

What if, instead of in the whole corpus, we wanted to get a bit more granular, considering each address? What are the most frequent five words in each?

```{r}
address_words %>%
  anti_join(stopwordlist) %>%
  group_by(filename,word)%>%
  summarize(count=n()) %>%
  arrange(filename,-count) %>%
  top_n(5)
```

Or maybe, at this level, we want to compare how long the speeches are. One way we could quantify length is word count. R can help us to do this very quickly.

```{r}
address_words %>%
  group_by(filename) %>%
  summarize(count = n()) %>%
  arrange(-count)
```

#### Word Lengths

While this is great in getting a sense of how long  How do we model how long a word is?

```{r}
address_word_length <- address_words %>%
  mutate(length = str_length(word))

address_word_length %>%
  group_by(filename) %>%
  summarize(mean_length = mean(length))
```

Great! But not so fast: sometimes, a calculation we've done at a distance may not be doing what we think it is. What could we do to check these results? (One option might be to filter by a single speech and calculate some values for that, cross-checking with the above results...)

```{r}

```

It's also kind of hard to compare values in a table. Let's plot it! Here, we might bring in the original tibble so we can incorporate other metadata so we can include the president's names in our plot. (I'm including `year` here in our `group_by` so that it will still appear in our summary, and we can order our plot chronologically.) Try to focus on the join here, rather than the grammar of the `ggplot` function — we'll be going over that in another lesson.

```{r}
left_join(address_word_length,addresses) %>%
  group_by(president,year) %>%
  summarize(mean_length = mean(length)) %>%
  ggplot(.,
         aes(x=year, y=mean_length)) +
  geom_point() +
  geom_smooth() +
  geom_text(aes(label=president,
                hjust=0,
                vjust=0))
  
```

What trends do we see? What stands out? What might we want to look into in more detail?

### Tokenizing by Sentence

We can, alternatively, tokenize by sentence to compare the sentence lengths in inaugural addresses. It's pretty easy to do this by adapting the structure of the code above, changing some variables. But we run into some friction: note that there's a couple more steps involved in figuring out the lengths of sentences, since we can't use `str_length` anymore. What might we do instead?

```{r}
address_sentences <- addresses %>% group_by(filename) %>%
  mutate(sentences = tokenize_sentences(text)) %>%
  select(filename,sentences)%>%
  unnest(sentences)

address_sentence_length <- address_sentences %>%
  group_by(filename,sentences) %>%
  mutate(word = tokenize_words(sentences)) %>% 
  unnest() %>%
  summarize(length = n()) %>%
  arrange(-length)
  
left_join(address_sentence_length,addresses) %>%
  group_by(president,year) %>%
  summarize(mean_length = mean(length)) %>%
  ggplot(.,
         aes(x=year, y=mean_length)) +
  geom_point() +
  geom_smooth() +
  geom_text(aes(label=president,
                hjust=0,
                vjust=0))
```

Wow, there are some *long* sentences in early addresses! How do these trends correspond with word length?

## Adding in New Metadata to Facet

It might be interesting to facet what we've done so far by political affiliation. Maybe presidents from some parties use certain words more or less than others in their addresses to the people, or are more likely to use longer sentences.

```{r}
presidential_affiliations <- read_csv(paste0(base_url,"affiliations.csv")) %>%
  rename(president = "PRESIDENT", term="TERM", party="POLITICAL PARTY")
```

Since our dataset only includes last names, we have to disambiguate between the Roosevelts to join these. We could do this by adding first names to our inaugural metadata, but instead, let's try matching them up by the years of each presidential administration. First, we'll need to use regular expressions to split the term (a date range) into start and end dates. (I brought in a new `dplyr` function called `case_when`. Can you infer how it works?)

We'll also need a package called `fuzzyjoin` in order to join using a range of years instead of just a single year, like we did in Workbook 3. (The syntax of this package's functions are a little funky.)

```{r}
presidential_affiliations <- presidential_affiliations %>%
  mutate(start = as.integer(str_replace_all(term,"(\\d{4})(-)?(\\d{4})?","\\1"))) %>%
  mutate(end = as.integer(str_replace_all(term,"(\\d{4})(-)?(\\d{4})?", "\\3"))) %>%
  mutate(end = case_when(is.na(end) ~ start,
                              TRUE ~ end
  )
           )

administrations <- presidential_affiliations %>%
  select(party,start,end)

library(fuzzyjoin)

addresses <- fuzzy_left_join(
  addresses,
  administrations,
  by = c(
    "year" = "start",
    "year" = "end"
  ),
  match_fun = list(`>=`, `<`)
) %>%
  select(filename,text,year,president,party)

addresses
```

Now that our `addresses` tibble includes party information, we could repeat all of the above with this new information. In the interest of time, let's move on to a different kind of analysis.

## Using a Lexicon: Sentiment Analysis

So far, our simple counts have quantified properties of these speeches' language without much regard to their content. While these formal aspects indeed carry meaning, distant analyses don't need to assume that all text contributes equally to meaning. One way that we can combat this problem is to assign, or code, meanings to certain words over others. We saw some of this processes when we removed stop words: there, we decided that function words were less important to our analysis than other words with more lexical meaning. 

Here, we're going to model this lexical meaning of words using *sentiment analysis*, which makes use of lexicons that assign a sentiment to each word in an amassed list of words. Ryan has had this to say about the method:

"Sentiment analysis is a method for tracing the emotional valences of texts. It does this, at base, by assigning an emotional valence to each word in a given text from a menu of possibilities. There are different SA algorithms that construe these possibilities differently, and there's a robust debate in computer science and related fields about which of these best represent the realities of language that SA models. Like any field, there are competing theories and methods, from which we will experiment with a few. But you should not construe the analyses we will conduct below as the only possibilities within the sentiment analysis field. Remember that humans design, debate, and modify algorithms: they are expressions of human intentions and desire for understanding, not impersonal structures descended from on high."

So how do we do it? Once again, `tidytext` has assembled helpful data for performing different types of text analysis. Here, let's call the NRC Word-Emotion Association Lexicon. (Note that using a lexicon like this starts by tokenizing at th word level. What assumptions does this make, and what information do we potentially "lose"?)

What possible sentiments are there?

```{r}
sentiments <- get_sentiments("nrc") # %>% filter(sentiment == "anger")

sentiments %>%
  group_by(sentiment) %>%
  summarize()
```

Joining a `sentiments` tibble we define to our `address_words` tibble will let us see the sentiments of these words.

We can use a left join to retain observations for inaugural address words that aren't in the lexicon. What words in our corpus are missing? Note, for example, that we already lose plurals. How to remedy that, or any other, is beyond our scope, but important questions to ask might be: (1) How many words does my corpus have that are not being represented, and how do I go about parsing my text for them? (2) To what degree does including those words or not affect the result of my analysis, given my particular corpus? We can 

```{r}
address_words %>%
  left_join(sentiments)

address_words %>%
  left_join(sentiments) %>%
  filter(is.na(sentiment))
```

Once we've done this preliminary assessment, we can start asking informed questions. What presidents have given speeches with the most angry words?

```{r}
address_words %>% 
  inner_join(addresses) %>%
  inner_join(sentiments) %>%
  filter(is.na(sentiment) == FALSE) %>%
  filter(sentiment == "anger") %>%
  group_by(president,sentiment) %>%
  summarize(count = n()) %>%
  arrange(-count)
```

Since these are inagural addresses, do we have a lot of anticipatory words? Using the above, write some code that will find the words that match that sentiment.

```{r}

```

Note, already, that there's overlap here. Can we think of the reason that absolute counts of words with certain sentiments might not be the best way to characterize sentiment with respect to the rest of the corpus? What might we do instead to regularize these frequencies?

```{r}

```

A potentially more interesting thing we might do is to plot the sentiment over each address. Let's load a differnet lexicon that codes words in a binary positive/negative model. Finally, we'll plot the 9 most recent addresses.

```{r}
## not working yet!!!!!! 

sentiments2 <- get_sentiments("bing")

test <- address_words %>%
  left_join(addresses) %>%
  inner_join(sentiments2) %>%
  count(filename, president, index = as.numeric(rownames(.)) %/% 80, sentiment) %>% #this is throwing an error (what's the floor divide 80 for?)
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

recent_addresses <- addresses %>%
  arrange(-year) %>%
  slice(1:9)

ggplot(test %>% filter(filename %in% recent_addresses$filename), 
       aes(index, sentiment, fill = filename)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~filename, ncol = 3, scales = "free_x")
```

## N-grams

Another way me might start to take into account the content of our text while modeling it is to . This may help us account for the assumptions involved in tokenizing at the word level, is to incorporate some more context for each word into our analysis. 

stylistic analysis (stock phrases between different presidents); 30-year increments

### Comparing N-grams across different texts

# Parsing XML data with R

There are at least two major situations I can think of that one can expect to be working with mark up. One is if you are working with the _TEI_. The _TEI_, or _Text-Encoding Initiative_, is an XML-language that is currently the de-facto standard for using markup to model texts in digital form. The other situation is if you are scraping content from the web — your browser builds and displays the content of the web pages you visit using html files, and html is a text-based markup language. Parsing

```{r}
library(xml2)



parsed <- dhq_file %>% read_xml()

```

I am far from an expert in XPath, and this is not a course in the language, but good resources exist for this if you are working with XML-based corpora.

```{r}

```


# Exercises

1. Together, we calculated the word counts of each individual inagural address. Can you express these counts as a ratio with the total word count of the corpus? How about the median word count across addresses? 

2. Together, we identified the most frequent words in the inaguration address corpus. Maybe some addresses make more use of these frequent words than others. Perhaps some addresses use radically different lexicon, or maybe the corpus makes consistent use of these words. Are there some addresses that have a higher proportion of the frequently-used words than others?

3. Together, we plotted the sentiment over each inaugural address, but maybe we want to see a bigger picture. What would be the most reasonable way, you think, of assigning a sentiment at the document level, based on what we know from our word-level analysis? It might be useful, for example, to characterize each address by a key sentiment. How do we do that most responsibly, given our knowledge of absolute and relative frequencies? Give it a try. Once you do, attempt to determine how sentiment of inaugural addresses have changed or not over the course of their delivery.

4. 

5. This one is for the folks working with XML data. So far, we've only went over loading in and parsing a single TEI file. Adapting the code that brings in our plaintext corpus from this notebook, can you bring your whole corpus into the R `tidyverse`? Your first step could be to get a tibble with two variables: the filenames/IDs and the parsed documents. From there, see if you can isolate a feature that's important to you across the corpus — whether it's the titles of the documents, creator and revision metadata, bodies of their texts, or something else.
